{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define the Neural Network Architecture ---\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_dim=2, output_dim=1, num_hidden_layers=4, num_neurons_per_layer=50):\n",
    "        super(PINN, self).__init__()\n",
    "        \n",
    "        layers = [nn.Linear(input_dim, num_neurons_per_layer), nn.Tanh()]\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(num_neurons_per_layer, num_neurons_per_layer))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(num_neurons_per_layer, output_dim))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Define the PINN Solver ---\n",
    "class PINN_WaveEquation_Implementation:\n",
    "    def __init__(self, c=1.0, num_hidden_layers=4, num_neurons_per_layer=50, learning_rate=0.001):\n",
    "        self.c = c\n",
    "        self.model = PINN(num_hidden_layers=num_hidden_layers, num_neurons_per_layer=num_neurons_per_layer)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss() # Mean Squared Error Loss\n",
    "\n",
    "        # Device configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "    \n",
    "        # Helper method to compute derivatives using torch.autograd.grad\n",
    "    def compute_derivatives(self, u, x, t):\n",
    "        # First derivatives\n",
    "        # We need create_graph=True for the first derivative to compute second derivative later\n",
    "        u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "        u_t = torch.autograd.grad(u, t, torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "        # Second derivatives\n",
    "        u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u_x))[0]\n",
    "        u_tt = torch.autograd.grad(u_t, t, torch.ones_like(u_t))[0]\n",
    "        \n",
    "        return u_x, u_t, u_xx, u_tt\n",
    "\n",
    "    def compute_loss(self, X_collocation, X_initial, X_boundary):\n",
    "        # Ensure inputs require gradients\n",
    "        x_c = X_collocation[:, 0:1].clone().detach().requires_grad_(True)\n",
    "        t_c = X_collocation[:, 1:2].clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # Predict u at collocation points\n",
    "        u_pred_c = self.model(torch.cat([x_c, t_c], dim=1))\n",
    "\n",
    "        # Compute derivatives for PDE loss\n",
    "        u_x, u_t, u_xx, u_tt = self.compute_derivatives(u_pred_c, x_c, t_c)\n",
    "        \n",
    "        # PDE residual (f_u = u_tt - c^2 * u_xx = 0)\n",
    "        pde_residual = u_tt - self.c**2 * u_xx\n",
    "        loss_pde = self.loss_fn(pde_residual, torch.zeros_like(pde_residual))\n",
    "\n",
    "        # --- Initial Conditions Loss ---\n",
    "        x_ic = X_initial[:, 0:1].clone().detach().requires_grad_(True)\n",
    "        t_ic = X_initial[:, 1:2].clone().detach().requires_grad_(True) # Should be all zeros\n",
    "        \n",
    "        u_pred_ic = self.model(torch.cat([x_ic, t_ic], dim=1))\n",
    "        # Compute u_t at IC points\n",
    "        u_t_pred_ic = torch.autograd.grad(u_pred_ic, t_ic, torch.ones_like(u_pred_ic), create_graph=True)[0]\n",
    "        \n",
    "        # u(x, 0) = sin(pi * x)\n",
    "        u_initial_true = torch.sin(torch.pi * x_ic)\n",
    "        loss_ic_u = self.loss_fn(u_pred_ic, u_initial_true)\n",
    "\n",
    "        # du/dt(x, 0) = 0\n",
    "        loss_ic_ut = self.loss_fn(u_t_pred_ic, torch.zeros_like(u_t_pred_ic))\n",
    "\n",
    "        # --- Boundary Conditions Loss ---\n",
    "        x_bc = X_boundary[:, 0:1].clone().detach().requires_grad_(True)\n",
    "        t_bc = X_boundary[:, 1:2].clone().detach().requires_grad_(True)\n",
    "        \n",
    "        u_pred_bc = self.model(torch.cat([x_bc, t_bc], dim=1))\n",
    "\n",
    "        # u(0, t) = 0 and u(1, t) = 0\n",
    "        loss_bc = self.loss_fn(u_pred_bc, torch.zeros_like(u_pred_bc))\n",
    "\n",
    "        # Total Loss (can add weights)\n",
    "        total_loss = loss_pde + loss_ic_u + loss_ic_ut + loss_bc\n",
    "        return total_loss, loss_pde, loss_ic_u, loss_ic_ut, loss_bc\n",
    "\n",
    "    def train(self, num_epochs, num_collocation_points, num_initial_points, num_boundary_points):\n",
    "        # Generate training points on CPU, then move to device\n",
    "        # Collocation points\n",
    "        X_collocation_np = np.random.rand(num_collocation_points, 2)\n",
    "        X_collocation_np[:, 0] = X_collocation_np[:, 0] * 1.0 # x in [0, 1]\n",
    "        X_collocation_np[:, 1] = X_collocation_np[:, 1] * 1.0 # t in [0, 1]\n",
    "        X_collocation = torch.from_numpy(X_collocation_np).float().to(self.device)\n",
    "\n",
    "        # Initial condition points (t=0)\n",
    "        X_initial_np = np.random.rand(num_initial_points, 2)\n",
    "        X_initial_np[:, 0] = X_initial_np[:, 0] * 1.0 # x in [0, 1]\n",
    "        X_initial_np[:, 1] = 0.0                     # t = 0\n",
    "        X_initial = torch.from_numpy(X_initial_np).float().to(self.device)\n",
    "\n",
    "        # Boundary condition points (x=0 or x=1)\n",
    "        X_boundary_left_np = np.random.rand(num_boundary_points // 2, 2)\n",
    "        X_boundary_left_np[:, 0] = 0.0               # x = 0\n",
    "        X_boundary_left_np[:, 1] = X_boundary_left_np[:, 1] * 1.0 # t in [0, 1]\n",
    "\n",
    "        X_boundary_right_np = np.random.rand(num_boundary_points // 2, 2)\n",
    "        X_boundary_right_np[:, 0] = 1.0              # x = 1\n",
    "        X_boundary_right_np[:, 1] = X_boundary_right_np[:, 1] * 1.0 # t in [0, 1]\n",
    "        X_boundary = torch.from_numpy(np.vstack((X_boundary_left_np, X_boundary_right_np))).float().to(self.device)\n",
    "        \n",
    "        history = {'total_loss': [], 'pde_loss': [], 'ic_u_loss': [], 'ic_ut_loss': [], 'bc_loss': []}\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss, loss_pde, loss_ic_u, loss_ic_ut, loss_bc = self.compute_loss(\n",
    "                X_collocation, X_initial, X_boundary\n",
    "            )\n",
    "            \n",
    "            total_loss.backward() # Backpropagation\n",
    "            self.optimizer.step() # Update model parameters\n",
    "\n",
    "            history['total_loss'].append(total_loss.item())\n",
    "            history['pde_loss'].append(loss_pde.item())\n",
    "            history['ic_u_loss'].append(loss_ic_u.item())\n",
    "            history['ic_ut_loss'].append(loss_ic_ut.item())\n",
    "            history['bc_loss'].append(loss_bc.item())\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}, Total Loss: {total_loss.item():.4e}, \"\n",
    "                        f\"PDE Loss: {loss_pde.item():.4e}, \"\n",
    "                        f\"IC_u Loss: {loss_ic_u.item():.4e}, \"\n",
    "                        f\"IC_ut Loss: {loss_ic_ut.item():.4e}, \"\n",
    "                        f\"BC Loss: {loss_bc.item():.4e}\")\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Starting PINN training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m pinn = PINN_WaveEquation_Implementation(c=wave_speed)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting PINN training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m history = \u001b[43mpinn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_collocation_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_initial_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boundary_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mPINN_WaveEquation_Implementation.train\u001b[39m\u001b[34m(self, num_epochs, num_collocation_points, num_initial_points, num_boundary_points)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     total_loss, loss_pde, loss_ic_u, loss_ic_ut, loss_bc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_collocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_boundary\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     total_loss.backward() \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.step() \u001b[38;5;66;03m# Update model parameters\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mPINN_WaveEquation_Implementation.compute_loss\u001b[39m\u001b[34m(self, X_collocation, X_initial, X_boundary)\u001b[39m\n\u001b[32m     33\u001b[39m u_pred_c = \u001b[38;5;28mself\u001b[39m.model(torch.cat([x_c, t_c], dim=\u001b[32m1\u001b[39m))\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Compute derivatives for PDE loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m u_x, u_t, u_xx, u_tt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_pred_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# PDE residual (f_u = u_tt - c^2 * u_xx = 0)\u001b[39;00m\n\u001b[32m     39\u001b[39m pde_residual = u_tt - \u001b[38;5;28mself\u001b[39m.c**\u001b[32m2\u001b[39m * u_xx\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mPINN_WaveEquation_Implementation.compute_derivatives\u001b[39m\u001b[34m(self, u, x, t)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Second derivatives\u001b[39;00m\n\u001b[32m     22\u001b[39m u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u_x))[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m u_tt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m u_x, u_t, u_xx, u_tt\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/PINN-wave-equation/lib/python3.12/site-packages/torch/autograd/__init__.py:502\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    498\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    499\u001b[39m         grad_outputs_\n\u001b[32m    500\u001b[39m     )\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    513\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    514\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    515\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/PINN-wave-equation/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# --- 3. Run the PINN ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    wave_speed = 1.0\n",
    "    num_epochs = 10000 # Increase for better accuracy, but takes longer\n",
    "    num_collocation_points = 10000\n",
    "    num_initial_points = 500\n",
    "    num_boundary_points = 500\n",
    "\n",
    "    # Initialize and train the PINN\n",
    "    pinn = PINN_WaveEquation_Implementation(c=wave_speed)\n",
    "    print(\"Starting PINN training...\")\n",
    "    history = pinn.train(num_epochs, num_collocation_points, num_initial_points, num_boundary_points)\n",
    "    print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 4. Visualize Results ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Plot loss history\u001b[39;00m\n\u001b[32m      4\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m plt.plot(\u001b[43mhistory\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtotal_loss\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mTotal Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m plt.plot(history[\u001b[33m'\u001b[39m\u001b[33mpde_loss\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mPDE Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m plt.plot(history[\u001b[33m'\u001b[39m\u001b[33mic_u_loss\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mIC (u) Loss\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 4. Visualize Results ---\n",
    "\n",
    "# Plot loss history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history['total_loss'], label='Total Loss')\n",
    "plt.plot(history['pde_loss'], label='PDE Loss')\n",
    "plt.plot(history['ic_u_loss'], label='IC (u) Loss')\n",
    "plt.plot(history['ic_ut_loss'], label='IC (du/dt) Loss')\n",
    "plt.plot(history['bc_loss'], label='BC Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('PINN Training Loss History (PyTorch)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Generate a grid for prediction\n",
    "x_grid = np.linspace(0, 1, 100)\n",
    "t_grid = np.linspace(0, 1, 100)\n",
    "X_plot, T_plot = np.meshgrid(x_grid, t_grid)\n",
    "XT_flat_np = np.vstack([X_plot.ravel(), T_plot.ravel()]).T\n",
    "XT_tensor = torch.from_numpy(XT_flat_np).float().to(pinn.device)\n",
    "\n",
    "# Predict u(x,t) using the trained PINN\n",
    "pinn.model.eval() # Set model to evaluation mode\n",
    "with torch.no_grad(): # Disable gradient calculations for inference\n",
    "    u_pred_flat = pinn.model(XT_tensor).cpu().numpy()\n",
    "u_pred = u_pred_flat.reshape(X_plot.shape)\n",
    "\n",
    "# Analytical solution for comparison\n",
    "u_analytical = np.sin(np.pi * X_plot) * np.cos(np.pi * T_plot)\n",
    "\n",
    "# Plotting 3D surface\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X_plot, T_plot, u_pred, cmap=cm.viridis)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('t')\n",
    "ax1.set_zlabel('u(x,t) (PINN)')\n",
    "ax1.set_title('PINN Solution of Wave Equation (PyTorch)')\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.plot_surface(X_plot, T_plot, u_analytical, cmap=cm.viridis)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('t')\n",
    "ax2.set_zlabel('u(x,t) (Analytical)')\n",
    "ax2.set_title('Analytical Solution of Wave Equation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting error\n",
    "error = np.abs(u_pred - u_analytical)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(error.T, extent=[0, 1, 1, 0], origin='lower', cmap='hot', aspect='auto')\n",
    "plt.colorbar(label='Absolute Error')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.title(f'Absolute Error |u_pred - u_analytical| (Max Error: {np.max(error):.4e})')\n",
    "plt.show()\n",
    "\n",
    "# Plotting snapshots in time\n",
    "plt.figure(figsize=(10, 8))\n",
    "times_to_plot = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "for i, t_val in enumerate(times_to_plot):\n",
    "    idx_t = np.argmin(np.abs(t_grid - t_val))\n",
    "    plt.subplot(len(times_to_plot), 1, i + 1)\n",
    "    plt.plot(x_grid, u_pred[idx_t, :], label=f'PINN at t={t_val:.2f}')\n",
    "    plt.plot(x_grid, u_analytical[idx_t, :], '--', label=f'Analytical at t={t_val:.2f}')\n",
    "    plt.title(f'u(x, {t_val:.2f})')\n",
    "    plt.legend()\n",
    "    plt.ylim([-1.1, 1.1])\n",
    "    if i == len(times_to_plot) - 1:\n",
    "        plt.xlabel('x')\n",
    "    else:\n",
    "        plt.xticks([]) # Hide x-ticks for intermediate plots\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Wave Snapshots Over Time', y=1.02)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PINN-example-YT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
